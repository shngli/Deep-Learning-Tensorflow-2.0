{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "KiqvBJQlDNPd",
    "outputId": "ae8784b3-3982-45eb-8e92-677af0ab2868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 348.9MB 45kB/s \n",
      "\u001b[K     |████████████████████████████████| 501kB 42.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.1MB 43.5MB/s \n",
      "\u001b[?25h2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "# Install TensorFlow\n",
    "# !pip install -q tensorflow-gpu==2.0.0-beta1\n",
    "\n",
    "try:\n",
    "  %tensorflow_version 2.x  # Colab only.\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "In this lecture we are going to emphasize the importance of shapes in RNN. Whenever you hear something like `NxTxD`, you should be automatically visualizing about a box and its dimensions. This lecture is all about tracking the shapes in an RNN, and also we are going to go through the RNN calculation manually to reinforce our understanding of how an RNN works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jl8zf2weDiLM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, SimpleRNN, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have listed out all the important size variables we have to pay attention to. These things should be permanently stored in your memory. You should never be asking what does `M` mean again.\n",
    "\n",
    "Just to recap, `N` is the number of samples in your dataset. `T` is the sequence length. Remember that in Tensorflow, we assume constant size sequences. \n",
    "\n",
    "`D` is the input feature dimensionality. We've gone through many examples of this where you might have a `D` > 1. `M` is the number of hidden units. This is the same as we have in a regular feed forward ANN, so it's a hyperparameter which you can choose. \n",
    "\n",
    "Finally, `K` is the number of output nodes. As a side note, `K` > 1 does not automatically imply you are doing classification with a softmax. You can do multi-dimensional regression too eg. you are trying to predict lat-long coordinates. In that scenario, `K` = 2 but it would still be a regression problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLxnQYNWFfjN"
   },
   "outputs": [],
   "source": [
    "# Things you should automatically know and have memorized\n",
    "# N = number of samples\n",
    "# T = sequence length\n",
    "# D = number of input features\n",
    "# M = number of hidden units\n",
    "# K = number of output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUM469Z7Dndx"
   },
   "outputs": [],
   "source": [
    "# Make some dummy data\n",
    "N = 1 # 1 sample\n",
    "T = 10 # sequence length = 10\n",
    "D = 3 # feature dimensionality = 3\n",
    "K = 2 # 2 output nodes\n",
    "X = np.random.randn(N, T, D) # input X has the shape N x T x D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our model. Here we also set `M`, the number of hidden units, to be 5. As usual, we start with an input layer whose shape is `TxD`, then we create a Simple RNN layer, which has the number of hidden units `M`. Let's assume the default activation, which is a TanH. Finally we create a dense layer with the number of output units `K`. For this I'll assume we're doing regression, so there is no activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8ZZ1E2TDxOt"
   },
   "outputs": [],
   "source": [
    "# Make an RNN\n",
    "M = 5 # number of hidden units\n",
    "i = Input(shape=(T, D))\n",
    "x = SimpleRNN(M)(i)\n",
    "x = Dense(K)(x)\n",
    "\n",
    "model = Model(i, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use our model to make a prediction. Obviously both our data and weights are random, so this prediction is not meaningful. These numbers are just for sanity checking. As you can see, the output shape is 1x2 as expected ie. we have 1 sample and 2 output nodes. Take notes of these numbers as this is what we want to compare with later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "hLjIp7eZEApH",
    "outputId": "c428b3f9-4ed6-44ac-ac15-d0121a946f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7062384   0.45167243]]\n"
     ]
    }
   ],
   "source": [
    "# Get the output\n",
    "Yhat = model.predict(X)\n",
    "print(Yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can do a model summary so that we can see all the layers of our RNN. As expected, We have three layers: the input layer, the Simple RNN layer and the dense layer. We don't exactly know what parameters are stored in a Simple RNN, although we do know the mathematical equation to get the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "V-3JmKEgEGc9",
    "outputId": "50f10128-1e76-4f20-c72b-2b1896bb7fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 10, 3)]           0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 57\n",
      "Trainable params: 57\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# See if we can replicate this output\n",
    "# Get the weights first\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the weights and see what they are. So we have some idea of what's stored in the Simple RNN layer. It looks like 3 big arrays, and it's actually more helpful to prints out the shape of these arrays. From there we can deduce which array corresponds to which weight in the Simple RNN. \n",
    "\n",
    "We get a 3x5, a 5x5, and a 5 length vector. If you recall `D`=3 and `M`=5, so that makes sense. The first weight is `DxM`, which means it's the input to hidden weight. The second weight is `MxM`, which means it's the hidden-hidden weight. And the third weight is a vector of length `M`, which means it's the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Q_FobnZqEbtv",
    "outputId": "7a665488-8ef6-4d02-ee96-7cee7433d6cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.06160122,  0.16070706,  0.83621055,  0.04993761, -0.36932853],\n",
       "        [ 0.4978891 , -0.474034  ,  0.55890614,  0.06967717,  0.21268493],\n",
       "        [-0.44685632, -0.28297323, -0.17539108,  0.42829865,  0.22275227]],\n",
       "       dtype=float32),\n",
       " array([[ 0.00272548,  0.04928541,  0.32022277,  0.3270029 ,  0.88774437],\n",
       "        [ 0.6996881 ,  0.64928424, -0.08133215, -0.27187836,  0.09128988],\n",
       "        [-0.22173485,  0.50949985,  0.6649476 ,  0.31805265, -0.38461757],\n",
       "        [ 0.5346833 , -0.24025255, -0.13355102,  0.7674074 , -0.22280595],\n",
       "        [ 0.41877976, -0.50861543,  0.65639263, -0.35927662, -0.07747886]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what's returned\n",
    "model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6FWsTeCJEjNc",
    "outputId": "43587cd2-841b-4c7d-9d18-bbdfaee46bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (5, 5) (5,)\n"
     ]
    }
   ],
   "source": [
    "# Check their shapes\n",
    "# Should make sense\n",
    "# First output is input > hidden\n",
    "# Second output is hidden > hidden\n",
    "# Third output is bias term (vector of length M)\n",
    "a, b, c = model.layers[1].get_weights()\n",
    "print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assign our weight variables with confidence. So for the layer at index 1, we assign these weights `Wx`, `Wh` and `bh`. Notice I'm using shorthand here so I don't use `WxH` and `WHH` since it's not that useful. \n",
    "\n",
    "For the layer at index 2, this corresponds to the output layer. So we assign these to the weights `Wo` and `bo`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCU8mjC2Ew8C"
   },
   "outputs": [],
   "source": [
    "Wx, Wh, bh = model.layers[1].get_weights()\n",
    "Wo, bo = model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to do our manual RNN calculation. To start, we are going to initialize the initial hidden state to a vector of 0s. By the way, if we get this wrong, then the output will be different. So here's another way we can confirm that the initial state really is 0. \n",
    "\n",
    "Next we get X at index 0, which is our one and only sample, we call this `x`. Next we initialize an empty list for all of our ŷ's. In this example, we only care about the final ŷ, but we are going to calculate them all for completeness. \n",
    "\n",
    "Next we enter into our loop where `t` counts up from 0 up to `T`. Inside the loop, we calculate the first `h`, that's the hidden value at the hidden layer. It's equal to `tanh(x[t].dot(Wx) + h_last.dot(Wh) + bh)`. You should recognize this formula from the lecture slides, so you should be cross referencing with those. Once we have `h` we can calculate ŷ, which is just the usual neuron equation. Finally we assign `h` to `h_last`, so that `h_last` has the correct value for the next iteration of the loop. \n",
    "\n",
    "And once we're outside the loop, we can print out the final value of the `Yhats` list. And hopefully this is equal to what we calculated before when we called `model.predict()`. So that's pretty awesome, we've confirmed that these are indeed the calculations that are done in the Simple RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Z7IT6dz8EV-B",
    "outputId": "fb0937f3-a3fd-4af2-e991-6ea1c3d76806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.70623848  0.45167215]\n"
     ]
    }
   ],
   "source": [
    "h_last = np.zeros(M) # initial hidden state\n",
    "x = X[0] # the one and only sample\n",
    "Yhats = [] # where we store the outputs\n",
    "\n",
    "for t in range(T):\n",
    "  h = np.tanh(x[t].dot(Wx) + h_last.dot(Wh) + bh)\n",
    "  y = h.dot(Wo) + bo # we only care about this value on the last iteration\n",
    "  Yhats.append(y)\n",
    "  \n",
    "  # important: assign h to h_last\n",
    "  h_last = h\n",
    "\n",
    "# print the final output\n",
    "print(Yhats[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that made this exercise simpler was that we only had a one sample. As a bonus exercise, you can use an N > 1. Modify this code so that it still produces the same result even when you have multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s42pMfj1FvnX"
   },
   "outputs": [],
   "source": [
    "# Bonus exercise: calculate the output for multiple samples at once (N > 1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
